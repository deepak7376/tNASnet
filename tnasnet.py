# -*- coding: utf-8 -*-
"""tNASnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yl3flXhYBNmjQ2bXVZcXZmhHfHXoSWHB
"""
import pandas as pd
#read the file
df = pd.read_csv("/Users/deepak/Documents/OpenSource/tNASnet/dataset/jena_climate_2009_2016.csv")

df.head()

TRAIN_SPLIT = 3000

uni_data = df['T (degC)']
uni_data.index = df['Date Time']
uni_data.head()

uni_data = uni_data.values

uni_train_mean = uni_data[:300].mean()
uni_train_std = uni_data[:300].std()

uni_data = (uni_data-uni_train_mean)/uni_train_std

univariate_past_history = 20
univariate_future_target = 0

#pip install tensorboardcolab

# from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback
# tbc=TensorBoardColab()

# univariate stacked lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from sklearn.model_selection import train_test_split



# split a univariate sequence
def split_sequence(sequence, n_steps):
  X, y = list(), list()
  for i in range(len(sequence)):
    # find the end of this pattern
    end_ix = i + n_steps
    # check if we are beyond the sequence
    if end_ix > len(sequence)-1:
      break
    # gather input and output parts of the pattern
    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
    X.append(seq_x)
    y.append(seq_y)
  return array(X), array(y)

# define input sequence
#raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]

# choose a number of time steps
n_steps = 20
# split into samples
X, y = split_sequence(uni_data, n_steps)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))

# train test split
x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
# # define model
# model = Sequential()
# model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))
# model.add(LSTM(50, activation='relu'))
# model.add(Dense(1))
# model.compile(optimizer='adam', loss='mse')
# # fit model
# history = model.fit(x_train, y_train, epochs=2, batch_size = 32, validation_data=(x_val, y_val),verbose=1)
          #verbose=1, callbacks=[TensorBoardColabCallback(tbc)])

#from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

mpl.rcParams['figure.figsize'] = (8, 6)
mpl.rcParams['axes.grid'] = False


# def create_time_steps(length):
#   return list(range(-length, 0))


# def show_plot(plot_data, delta, title):
#   labels = ['History', 'True Future', 'Model Prediction']
#   marker = ['.-', 'rx', 'go']
#   time_steps = create_time_steps(plot_data[0].shape[0])
#   if delta:
#     future = delta
#   else:
#     future = 0

#   plt.title(title)
#   for i, x in enumerate(plot_data):
#     if i:
#       plt.plot(future, plot_data[i], marker[i], markersize=10,
#                label=labels[i])
#     else:
#       plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
#   plt.legend()
#   plt.xlim([time_steps[0], (future+5)*2])
#   plt.xlabel('Time-Step')
#   return plt

# show_plot([x_train[0], y_train[0]], 0, 'Sample Example')

#pip install deap

import random as rnd
from deap import base
from deap import creator
from deap import tools

IND_SIZE = 8
creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
creator.create("Individual", list, fitness=creator.FitnessMin)

toolbox = base.Toolbox()
toolbox.register("attr_pre_abs", (lambda : rnd.choice([0, 1])))
toolbox.register("attr_units", (lambda : rnd.choice([4, 8, 16, 32, 64, 128])))

func_seq = [toolbox.attr_pre_abs, 
            toolbox.attr_units]
toolbox.register("individual", tools.initCycle, creator.Individual,
                 func_seq, n=IND_SIZE)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten
import matplotlib.pyplot as plt

import tensorflow as tf

def evaluate(individual):

    num_params = len(func_seq)
    input_layer_flag = False
    return_flag = False

    model = Sequential() 

    for i in range(IND_SIZE):
        
        index = i*num_params
        
        if individual[index] > 0:
            if input_layer_flag==False:
                model.add(LSTM(individual[index+1],activation='relu',
                            input_shape=(n_steps, n_features),
                                return_sequences=True))
        
                input_layer_flag=True
        
            else:
                model.add(LSTM(individual[index+1],activation='relu', 
                            return_sequences=True))
        
            return_flag=True
    
    # final layer
    model.add(Flatten())
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    
    hist = model.fit(x_train, y_train, batch_size = 32, 
                     validation_data= (x_val, y_val) ,
                     epochs=2 ,verbose=2)
    
    print(hist.history['val_loss'])
    return hist.history['val_loss'][1],

def mutate(individual, indpb = 0.5):
    if rnd.random() < indpb:
        individual[0] = toolbox.attr_pre_abs()

    if rnd.random() < indpb:
        individual[1] = toolbox.attr_units()

toolbox.register("evaluate", evaluate)
toolbox.register("mutate", mutate, indpb = 0.15)
toolbox.register("crossover", tools.cxTwoPoint)
toolbox.register("select", tools.selTournament, tournsize = 3)

CXPB, MUTPB, NGEN, POPSIZE = 1, 0.2, 2, 4

pop = toolbox.population(n=POPSIZE)

print("\n--Initialization--")

# Evaluate the entire population
fitnesses = list(map(toolbox.evaluate, pop))
for ind, fit in zip(pop, fitnesses):
    ind.fitness.values = fit

# Begin the evolution
for g in range(NGEN):
    print("-- Generation:%i --" % g)
    
    # Select the next generation individuals
    offspring = toolbox.select(pop, len(pop))

    # Clone the selected individuals
    offspring = list(map(toolbox.clone, offspring))

    # Apply crossover and mutation on the offspring
    for child1, child2 in zip(offspring[::2], offspring[1::2]):

        # cross two individuals with probability CXPB
        if rnd.random() < CXPB:
            c1 = toolbox.clone(child1)
            c2 = toolbox.clone(child2)
            toolbox.crossover(child1, child2)
            # fitness values of the children
            # must be recalculated later
            if c1!=child1: del child1.fitness.values
            if c2!=child2: del child2.fitness.values

    for mutant in offspring:
        # mutate an individual with probability MUTPB
        if rnd.random() < MUTPB:
            #print("mut")
            #print(mutant)
            m1 = toolbox.clone(mutant)
            toolbox.mutate(mutant)
            if m1!=mutant: del mutant.fitness.values
    
    # Evaluate the individuals with an invalid fitness
    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]
    print(invalid_ind)
    fitnesses = map(toolbox.evaluate, invalid_ind)
    for ind, fit in zip(invalid_ind, fitnesses):
        ind.fitness.values = fit

    # print("  Evaluated %i individuals" % len(invalid_ind))
    # print(invalid_ind)
    
    # The population is entirely replaced by the offspring
    pop[:] = offspring
    
    # Gather all the fitnesses in one list and print the stats
    fits = [ind.fitness.values[0] for ind in pop]
    
    length = len(pop)
    mean = sum(fits) / length
    sum2 = sum(x*x for x in fits)
    std = abs(sum2 / length - mean**2)**0.5
    best_ind = tools.selBest(pop, POPSIZE)[0]

    print("Best individual is %s, %s" % (best_ind, best_ind.fitness.values))
    print("  Min %s" % min(fits))
    print("  Max %s" % max(fits))
    print("  Avg %s" % mean)
    print("  Std %s" % std)
    print("-- End of (successful) evolution --")